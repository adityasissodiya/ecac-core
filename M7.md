Locked. M7 = **metrics + evaluation harness**. No new features—just measuring what we already built and producing artifacts you can drop into the paper.

# M7 — Metrics & Evaluation Harness (convergence, rollback, replay cost)

## Goal

Instrument the prototype and ship a **repeatable bench harness** that generates CSV/JSON artifacts for the paper:

* **Convergence latency** after partitions heal
* **Rollback rate/cost** under deny-wins
* **Replay cost** (full vs incremental)
* **Network sync stats** (batches, duplicates) if M6 is enabled

No persistence/networking changes beyond reading existing counters; everything remains deterministic.

---

## Scope

**In**

* Structured metrics in code (counters, histograms, timers)
* Deterministic CSV/JSON exporters
* A small **workload driver** (single-process or multi-node) to run canned scenarios
* Plot-ready outputs (CSV + optional PNGs via a script)

**Out**

* Fancy dashboards, Prometheus/Grafana, long-term storage
* Adaptive tuning; this is measurement, not optimization

---

## What to measure (frozen)

### Replay & state

* `replay_full_ms`, `replay_incremental_ms`
* `ops_total`, `ops_applied`, `ops_skipped_policy`
* `mvreg_concurrent_winners` (avg, p95), `orset_tombstones_total`

### Policy

* `revocations_seen`, `ops_invalidated_by_revoke`
* `epoch_build_ms`, `epochs_total`

### Sync (if running M6)

* `gossip_announces_sent/recv`
* `fetch_batches`, `ops_fetched`, `ops_duplicates_dropped`
* `convergence_ms` (time from partition heal to stable identical state)

### Store (from M5)

* `batch_write_ms`, `checkpoint_create_ms`, `checkpoint_load_ms`

---

## Deliverables

### Code

* `crates/core/src/metrics.rs`

  * Tiny wrapper with **no global clock dependence**:

    ```rust
    pub struct Metrics { /* counters and histograms */ }
    impl Metrics {
      pub fn inc(&self, key: &'static str, by: u64);
      pub fn observe_ms(&self, key: &'static str, ms: u64);
      pub fn snapshot_csv(&self) -> String;
      pub fn reset(&self);
    }
    ```
  * Backed by lock-free atomics + fixed buckets; produce **stable column order**.

* Instrumentation points:

  * `replay.rs`: time full/incremental, count applied/skipped ops
  * `policy.rs`: time epoch build, count revocations/invalidations
  * `net/sync.rs`: count announces, batches, fetched ops, dup drops (guarded by `#[cfg(feature="net")]`)
  * `store/*`: time batch writes and checkpoint ops

* `crates/cli/src/bench.rs` — **evaluation harness**

  * Scenarios (deterministic RNG seeded):

    1. **HB chain** (baseline, no concurrency, no revokes)
    2. **Concurrent writers** (MVReg conflicts + OR-Set add/rem races)
    3. **Offline-revocation** (user edits offline, revoke concurrent → deny-wins)
    4. **Three-way partition** (requires `--net` flag to use M6)
  * Flags:

    ```
    bench run --scenario <name> --seed <u64> --ops <N> --peers <M> \
      [--partition schedule.json] [--net] [--checkpoint_every K]
    ```
  * Emits:

    * `/docs/eval/out/<scenario>-<seed>.csv` (metrics summary)
    * `/docs/eval/out/<scenario>-<seed>-timeline.jsonl` (time-stamped events)
    * `/docs/eval/out/<scenario>-<seed>-state.json` (final state export)

* `scripts/plot.py` (optional):

  * Reads CSVs; outputs PNGs: convergence CDF, replay cost vs N ops, rollback count vs revoke rate.

### Docs

* `docs/evaluation-plan.md` — exact scenarios, metrics definitions, and how to reproduce.
* `docs/eval/README.md` — one-liners to regenerate artifacts for the paper.

---

## Acceptance criteria (binary)

1. **Reproducibility**

   * Running `scripts/reproduce.sh` regenerates identical CSV/JSON for the same seed and commit hash.

2. **Convergence metric correctness**

   * In a 3-node partition/heal, `convergence_ms` is 0 when no new ops are exchanged; >0 when there are missing ops; repeated runs with same seed produce the same value.

3. **Rollback accounting**

   * In offline-revocation scenario, `ops_skipped_policy = ops_by_revoked_after(T_rev)`, and final state matches oracle replay.

4. **Replay parity**

   * `replay_full_ms` ≥ `replay_incremental_ms` for the same op set; states identical.

5. **No timing side effects**

   * Adding metrics does **not** change topo order or state (byte-identical before/after instrumentation).

6. **Stable schemas**

   * CSV columns and JSON keys are fixed; version stamped in file header.

---

## Test plan

**Unit tests**

* `metrics_snapshot_columns_stable`: snapshot returns fixed header order.
* `metrics_no_clock_leak`: timers driven by injected `Instant`/test clock.
* `bench_determinism`: given seed S, repeated runs write identical CSV/JSON (hash compare).

**Scenario tests (CLI)**

* `bench hb-chain`: expect 0 policy skips, low replay_ms, no dup drops.
* `bench concurrent`: MVReg winners > 1 in some fields; deterministic projection stable.
* `bench offline-revoke`: non-zero `ops_skipped_policy`; final state sans revoked ops.
* `bench partition-3`: with `--net`, non-zero `ops_fetched`, convergence_ms bounded; without `--net`, no convergence (expected).

**Property tests**

* Random seeds; assert invariants:

  * `ops_applied + ops_skipped_policy == ops_total`
  * If no revokes, `ops_skipped_policy == 0`
  * If no concurrency, `mvreg_concurrent_winners == 1`

---

## Implementation notes

* **Clocking**: For timers, inject a monotonic clock handle; in tests, use a fake clock so results are deterministic. In real runs, use `Instant::now()` but still log seed & commit.

* **CSV format**:

  * First line: `# ecac-metrics v1, commit=<hash>, scenario=<name>, seed=<seed>`
  * Second line: header (fixed order)
  * Single summary row; per-interval series go to JSONL timeline instead of inflating CSV.

* **Timeline JSONL**:

  * Events: `{"t_ms":123,"type":"op_applied","id":"..."}`
  * Keep it small; it’s for debugging plots and verifying convergence.

* **Seeding**:

  * One RNG seed drives: op payload generation, parent selection (concurrency), partition schedule (if `--net`), and revocation times. Persist it in the CSV header.

* **Feature flags**:

  * Build with `--features net` to enable network metrics; otherwise bench runs purely local.

---

## Non-goals

* Live dashboards
* Long-running endurance tests
* WAN-scale profiling

---

## Task checklist

* [ ] `metrics.rs` (API + atomic counters/histos + snapshot)
* [ ] Instrument replay/policy/store/net paths
* [ ] `cli bench` with scenarios + seeded RNG
* [ ] Exporters: CSV summary + JSONL timeline + final state JSON
* [ ] Docs: evaluation plan + reproduce instructions
* [ ] Tests: unit, scenario, property
* [ ] (Optional) plotting script & saved PNGs for the paper

---

## Risks / foot-guns

* **Nondeterministic timers** → inject clock in tests; don’t mix wall-clock with HLC.
* **CSV drift** → lock schema and add a test that diffs against a golden header.
* **Instrumentation overhead** → measure it once; if needed, guard heavy timers with a feature flag (but keep default on for paper runs).

Deliver M7 and you’ll have hard numbers and artifacts to back every performance/safety claim in the paper.
