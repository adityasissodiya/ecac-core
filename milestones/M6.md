Locked. M6 is where we make replicas *talk*—but still correctness-first, minimal surface area.

# M6 — Gossip Sync (libp2p), Anti-Entropy, and Deterministic Integration

## Goal (what M6 proves)

Two or more nodes can exchange ops over the network, recover from partitions, and **converge to the same policy-correct state** using the existing deterministic replay. Networking must not compromise determinism: given the same final op set, all nodes end with **identical** state, regardless of delivery order or duplicates.

---

## Scope

**In**

* **libp2p** (Rust) with **Noise** secure channels and **Gossipsub**.
* A minimal **sync protocol**:

  * Pubsub topic for **ANNOUNCE** (heads/summary).
  * Request/response for **FETCH** (get missing ops by ID).
* **Dedup**, **causal completion** (fetch parents first), and **signature verification** on ingest.
* **Anti-entropy round** (periodic compare & repair) to handle dropped gossip.
* Configurable **static peers** (no DHT/NAT traversal in M6).
* Integration with **store** (M5) + **replay** (M2–M4).

**Out**

* DHT discovery, NAT hole-punching, relay.
* Flow control across WAN; just local/LAN/simple cloud is fine.
* Encryption-at-rest; already out.

---

## Protocol (frozen for M6)

### Topics

* `ecac/v1/<project-id>/announce`
  Payload: `Announce { node_id, topo_watermark, head_ids[K], bloom16 }`

  * `head_ids`: up to K tip op_ids (ops with no children known locally)
  * `bloom16`: tiny Bloom over recent N op_ids (helps short-circuit fetch)

### RPC (tonic or libp2p request/response)

* `FetchMissing { want: Vec<OpId> } -> Stream<Op>`
  Sender must return ops **and all ancestors** until a supplied **have-set** boundary.

### Messages (CBOR, canonical)

* All messages signed by node key (separate from user keys) → **authentic sender**, but **we still verify per-op signatures**.

---

## Deliverables

### Code (new/changed)

* `crates/net/src/gossip.rs`

  * `Gossip::new(config)` with libp2p Swarm + Gossipsub.
  * Publish/handle `Announce`.
  * On receive: run **SyncPlanner** (see below).

* `crates/net/src/rpc.rs`

  * `fetch_missing(op_ids) -> stream<Op>` (server + client)
  * Server reads from `store`; returns canonical op bytes.

* `crates/net/src/sync.rs` (the brain)

  * `SyncPlanner::plan(local_heads, remote_heads, bloom) -> FetchPlan`
  * `FetchPlan` computes **missing frontier** by set difference and parent-closure.
  * Drives batched fetch; enforces **causal parent-first** insertion into `store`.

* `crates/store/` (small additions)

  * `Store::heads(K) -> Vec<OpId>` (tips with no known children)
  * `Store::recent_bloom(N) -> Bloom16`
  * `Store::contains(id) -> bool`

* `crates/core/` (no logic change)

  * Replay API already deterministic; add a convenience `on_new_ops_applied` hook for the CLI/UI.

* `crates/cli/`

  * `node --listen <addr> --peer <addr>... --project <id>`
  * `status` (shows op count, heads, topo index)
  * `inject-demo-ops` to simulate activity/partitions

### Docs

* `docs/architecture.md` → “Networking & Sync” section (sequence diagram).
* `docs/protocol.md` → Announce/FETCH schemas, invariants (idempotence, causal completion).

---

## Invariants & Acceptance Criteria (binary)

1. **Convergence across peers**
   Start N≥3 nodes, partition them arbitrarily, inject ops on each side, heal → all nodes converge to **byte-identical** JSON state.

2. **Determinism under reordering/dup**
   Flood with out-of-order and duplicate deliveries → final state identical to oracle (topo-sorted union of ops).

3. **Causal completeness**
   No node applies an op before all its parents are present. (Store must show parent presence for any applied op.)

4. **Integrity**
   Every received op must verify (hash + ed25519). Invalid ops are dropped; peers are not poisoned.

5. **Liveness (anti-entropy)**
   If an `Announce` is missed, periodic anti-entropy (T seconds) still syncs all ops.

6. **Isolation**
   A node never surfaces an op to replay if fetch returned a truncated chain (defensive check).

---

## Test Plan

**Unit / component**

* `planner_diff_small`: local heads vs remote heads → correct missing set.
* `parent_first_enforced`: planner always requests ancestors before children.
* `bloom_short_circuit`: when bloom says “have”, planner skips redundant fetches.

**Integration (single process, multiple swarms)**

* `two_node_sync`: A makes 50 ops, B makes 50 ops, then connect → converge.
* `three_way_partition`: A—B partitioned from C, all produce ops; later heal → converge.
* `duplicate_storm`: spam Announce + duplicate FETCH responses → dedup holds, state stable.
* `invalid_op_drop`: inject an op with bad signature → dropped, no crash.

**Property tests**

* Random partition schedules + random op DAGs over 3–5 nodes:

  * Assert convergence, no parent-before-child violation, and identical final state to oracle union.

**Manual / CLI**

* Run two nodes, inject revoke+data race on opposite nodes; watch deny-wins resolve identically after heal.

---

## Implementation Notes (don’t shoot yourself)

* **No “exactly-once” assumption.** Gossipsub can duplicate; we rely on `store.contains(op_id)` for dedup.
* **Parent-closure fetch:** when remote announces `head_ids`, fetch **backwards** until hitting an op you have (or genesis). Do it in **topo layers** to keep memory bounded.
* **Batching:** FETCH in bounded batches (e.g., 512 ops), commit each batch with a **single synced WriteBatch** (reuse M5).
* **Backpressure:** simple in M6—limit concurrent FETCH to 1 per peer to avoid blowups.
* **Security:** verify op signatures **before** writing to store; drop message otherwise. Node-level Announce signatures are informational; do not trust them for op integrity.
* **Project scoping:** include `<project-id>` in topic names and as a field in ops (already signed) to prevent cross-project contamination.
* **Clock discipline:** do **not** touch wall clock on network paths; HLC is contained in op payload already.

---

## Non-Goals

* Fancy peer discovery; use `--peer` static list.
* WAN scaling and QoS; M7 can add DHT/relay if needed.
* Membership/ACL for nodes; we trust peers configured in `--peer` for transport, but **still verify per-op authorship**.

---

## Task Checklist

* [ ] Implement `announce` publisher (periodic and on new ops)
* [ ] Implement `sync planner` (diff → parent-closure → batches)
* [ ] Implement `fetch` RPC (server/client)
* [ ] Wire ingestion path: verify → store.put_op → (optional) replay.incremental
* [ ] CLI node with static peers; status endpoint
* [ ] Tests: unit, integration, property as above
* [ ] Docs update (protocol + architecture)

---

## Risks / Foot-guns

* **Fetching children first** leads to gaps → enforce parent-first at planner and at store (reject orphan apply).
* **Rendezvous storms**: suppress Announce echo (don’t republish an Announce you just received).
* **State flapping**: replay incrementally only after batch commit; otherwise you’ll project half-batches.
* **Topic bleed**: always check `<project-id>` in message before acting.

Nail M6 and you’ve got a small but real distributed system: crash-safe, policy-correct, and partition-tolerant. M7 can then tackle peer discovery/NAT or read-encryption, depending on what you want to prove next.
