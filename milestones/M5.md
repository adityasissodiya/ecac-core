Awesome — at this point, making it durable matters more than making it distributed. Let’s lock down persistence so crashes/restarts don’t change answers.

# M5 — Durable Log, Indexes, and Checkpoints (RocksDB; no networking)

## Goal (what M5 proves)

You can **persist** the op log and policy/VC artifacts, **rebuild** the causal DAG and materialized state after a restart, and **checkpoint/restore** deterministically. Crashes don’t corrupt the log; replays from disk produce **bit-for-bit identical** state as in-memory runs from M1–M4.

---

## Scope (what’s in vs out)

**In**

* Embedded store using **RocksDB** with explicit column families.
* **Append-only op store** (immutable once committed).
* **Indexes** for DAG edges and per-entity lookups.
* **Verified VC cache** persistence (so we don’t re-verify on every boot).
* **Checkpoints**: periodic materialized-state snapshots + replay cursor.
* **Crash-safe writes** with `WriteBatch` (+ `sync=true` for correctness path).
* Tools to **replay from disk**, **verify integrity**, **dump state**.

**Out**

* Networking/libp2p, replication.
* Compaction/GC of old ops (we’ll add a GC plan but not implement aggressive pruning).
* Fancy encryption-at-rest (can be future).

---

## Storage layout

Use a RocksDB database with these **column families (CFs)**:

* `ops`
  Key: `op_id` (32B) → Value: **canonical CBOR** of the full op (as signed).
  *Immutable* after write.

* `edges`
  Key: `op_id` → Value: packed `parents[]` (array of 32B ids).
  (Redundant with `ops`, but speeds DAG rebuild without decoding.)

* `by_author`
  Key: `(author_pk || hlc || op_id)` → Value: empty.
  For quick range scans per author & time.

* `meta`
  Keys like: `schema_version`, `last_checkpoint_id`, `topo_watermark` (highest applied topo idx), `db_uuid`.

* `vc_raw`
  Key: `cred_hash` → Value: raw VC bytes (JWT compact form).

* `vc_verified`
  Key: `cred_hash` → Value: `VerifiedVc` CBOR (issuer, role, scope, nbf, exp).
  Only after successful verification in M4.

* `policy_index`
  Optional helper: `(subject_pk || role || tag || interval_id)` → encoded interval.
  Speeds epoch queries across restarts.

* `checkpoints`
  Key: `checkpoint_id` (Lamport/monotonic) → Value: snapshot blob (deterministic encoding of `State`) + `topo_index` at snapshot.

*All CFs are opened with checksums and `paranoid_checks=true`.*

---

## API surface (new/extended)

In `crates/store/`:

* `Store::open(path: &Path, opts: StoreOptions) -> Store`
* `Store::put_op(op: &Op) -> Result<()>`

  * Validates signature & hash, ensures parents are either present or marks **pending**.
  * Writes to `ops`, `edges`, `by_author` in a single synced `WriteBatch`.
* `Store::has_op(id) -> bool`, `Store::get_op(id) -> Option<Op>`
* `Store::iter_ops() -> impl Iterator<Item=OpId>` (sorted by key)
* `Store::topo_iter() -> impl Iterator<Item=OpId>`

  * Rebuild topo order using `edges` + Kahn (or a persisted topo order if cached).
* `Store::persist_vc(cred_hash, raw_bytes)` → writes `vc_raw`
* `Store::persist_verified_vc(cred_hash, verified)` → writes `vc_verified`
* `Store::build_auth_epochs() -> EpochIndex`

  * Recomputes from policy ops and `vc_verified` (can be cached in `policy_index`).
* `Checkpoint::{create, list, load, delete}`

  * `create(state: &State, topo_idx: u64) -> checkpoint_id`
  * `load(checkpoint_id) -> State`
  * `latest() -> Option<(checkpoint_id, topo_idx)>`

In `crates/core/`:

* Extend `replay::apply_incremental` to accept a **source** that yields either:

  * `TopoRange { from_idx, to_idx }` from store, or
  * an in-memory slice (as before).

In `cli/`:

* `op-append <file.cbor>` (batch append)
* `replay-from-store` (full or from checkpoint)
* `checkpoint-create` / `checkpoint-list` / `checkpoint-load`
* `verify-store` (run integrity checks: signatures, parent presence, topo validity)
* `dump-state-json` (deterministic export)

---

## Determinism rules (pin these down)

* **On-disk op bytes** are *exactly* the canonical CBOR you hash/signed in M1. Never re-encode on write path.
* **Topo order**: recomputed from `edges` every process start, with the same `(parents-first; tie: (hlc, op_id))`. Optionally cache the resulting `(op_id -> topo_idx)` map in a transient CF, but **never** make it source of truth.
* **Checkpoints**: encode `State` with a deterministic serializer (sorted maps/sets). Store alongside the `topo_idx` it represents. On restore, resume incremental apply from `topo_idx+1`.

---

## Acceptance criteria (binary)

1. **Crash-recovery correctness**
   Kill the process mid-append (after write, before checkpoint). On restart, **no op is lost or corrupted**, and full replay yields the same state as the pre-crash in-memory state.

2. **Bit-for-bit determinism**
   Given identical DB contents, `replay-from-store` produces **identical JSON** state across runs/hosts.

3. **Integrity**

   * Every stored op verifies (signature + hash).
   * Every `edges` entry matches the decoded parents from `ops`.
   * Topo iterator never yields a child before any of its parents.

4. **Epoch & policy persistence**
   After restart, rebuilding epochs from `vc_verified` and policy ops yields the **same allowed/denied** decisions as before.

5. **Checkpoint parity**
   `replay_full` from genesis equals `load(latest_checkpoint) + apply_incremental` from `topo_idx+1`.

6. **Pending parents**
   Ops appended with missing parents are not surfaced by `topo_iter()` until parents arrive; once parents are present, they appear in correct order without duplication.

---

## Test plan

**Unit tests (store)**

* `append_and_get`: write N ops, read back, byte-equal to originals.
* `parent_missing_then_arrives`: append child first, then parents; topo yields correct order.
* `integrity_scan`: corrupt a byte on disk → `verify-store` fails as expected.

**Crash tests**

* Use a test helper that forks a child, appends a batch with `sync=true`, `kill -9` mid-run; reopen DB, verify no partial/truncated entries (RocksDB guarantees atomic batch).

**Replay parity**

* Build a mixed data+policy+VC history in memory; store it; close; reopen; `replay-from-store` matches in-memory result (byte-wise).

**Checkpoint tests**

* Create checkpoint at K; mutate further; reload checkpoint; incremental apply → equals full replay.

**Property tests**

* Random DAGs with random crash injection points:

  * After recovery, `verify-store` passes and final state matches oracle replay.

---

## Implementation notes / guardrails

* **RocksDB options** (correctness path):

  * `create_if_missing=true`, `paranoid_checks=true`
  * `wal_dir` on same disk; `use_fsync=true` or `bytes_per_sync` with `sync=true` on batches.
  * **Write path**: always `WriteBatch` + `DB::write_opt(..., sync=true)`.
* **Schema versioning**:

  * Store `meta/schema_version = "ecac:v1"`. Refuse to open unknown versions; add migrations later.
* **IDs**:

  * Generate a `db_uuid` at init and store in `meta` for audit/debug.
* **Pending set**:

  * Maintain a CF or in-memory index mapping `missing_parent -> Vec<child_op_id>`. Persist minimal hints if needed; acceptable to recompute on boot by scanning `edges`.
* **VC caches**:

  * `vc_verified` is a **cache**: you can recompute from `vc_raw` + trust/status files. Treat mismatch as an integrity issue; prefer recompute in `verify-store`.
* **Auditing**:

  * Append a tiny “store event” record (optional) for checkpoint created/loaded (not cryptographically signed in M5).

---

## Non-goals

* No compaction GC of old ops beyond RocksDB’s own compaction.
* No encryption-at-rest.
* No online status/issuer fetch; trust/status remain local files.

---

## Task checklist

* [ ] Define CF schema; migration guard in `meta`
* [ ] Implement `Store::open`, `put_op`, `get_op`, `topo_iter`
* [ ] Persist VC raw/verified; wire into epoch rebuild
* [ ] Implement checkpoints (create/load/list)
* [ ] CLI: append, replay-from-store, verify-store, dump-state, checkpoints
* [ ] Tests: unit, crash, parity, property
* [ ] Docs: `docs/architecture.md` (“Persistence & Recovery”), `docs/protocol.md` (on-disk encoding)

---

## Risks / foot-guns

* **Re-encoding drift**: if you ever re-serialize ops before hashing, IDs diverge. Store and re-use the exact canonical bytes from M1.
* **Topo caching as truth**: don’t. Always rebuild from edges; cached topo is advisory only.
* **Partial writes**: only ever commit with a single synced `WriteBatch`.
* **Clock bleed**: don’t consult wall time during replay; only read fields in ops/VCs.

Once M5 is solid, M6 can layer **gossip sync** on top (libp2p Gossipsub), confident that disk state is authoritative and replay is deterministic.
